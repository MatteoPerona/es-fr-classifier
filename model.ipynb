{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import itertools\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_words, test_words, train_labels, test_labels = train_test_split(\n",
    "    data.word, data.label, test_size=0.2, random_state=42)\n",
    "\n",
    "train_words = list(train_words)\n",
    "test_words = list(test_words)\n",
    "train_labels = list(train_labels)\n",
    "test_labels = list(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    \n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        root: Root Node of the tree.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "        n_features: Number of features to use during building the tree.(Random Forest)\n",
    "        n_split:  Number of split for each feature. (Random Forest)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth = 1000, size_allowed = 1, n_features = None, n_split = None):\n",
    "        \"\"\"\n",
    "            Initializations for class attributes.\n",
    "        \"\"\"\n",
    "        self.root = None             \n",
    "        self.max_depth = max_depth         \n",
    "        self.size_allowed = size_allowed      \n",
    "        self.n_features = n_features        \n",
    "        self.n_split = n_split           \n",
    "    \n",
    "    \n",
    "    class Node():\n",
    "        \"\"\"\n",
    "            Node Class for the building the tree.\n",
    "\n",
    "            Attribute: \n",
    "                threshold: The threshold like if x1 < threshold, for spliting.\n",
    "                feature: The index of feature on this current node.\n",
    "                left: Pointer to the node on the left.\n",
    "                right: Pointer to the node on the right.\n",
    "                pure: Bool, describe if this node is pure.\n",
    "                predict: Class, indicate what the most common Y on this node.\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, threshold = None, feature = None):\n",
    "            \"\"\"\n",
    "                Initializations for class attributes.\n",
    "            \"\"\"\n",
    "            self.threshold = threshold   \n",
    "            self.feature = feature    \n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.pure = None\n",
    "            self.depth = None\n",
    "            self.predict = None\n",
    "    \n",
    "    \n",
    "    def entropy(self, lst):\n",
    "        \"\"\"\n",
    "            Function Calculate the entropy given lst.\n",
    "            \n",
    "            Attributes: \n",
    "                entro: variable store entropy for each step.\n",
    "                classes: all possible classes. (without repeating terms)\n",
    "                counts: counts of each possible classes.\n",
    "                total_counts: number of instances in this lst.\n",
    "                \n",
    "            lst is vector of labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        entro = 0  \n",
    "        classes = set(lst)  \n",
    "        counts = [list(lst).count(c) for c in classes] \n",
    "        total_counts = len(lst)    \n",
    "        for count in counts:       \n",
    "            if count != 0:    \n",
    "                p = count/total_counts    \n",
    "                entro -= p * np.log2(p)\n",
    "        return entro\n",
    "\n",
    "    def information_gain(self, lst, values, threshold):\n",
    "        \"\"\"\n",
    "            Function Calculate the information gain, by using entropy function.\n",
    "            \n",
    "            lst is vector of labels.\n",
    "            values is vector of values for individule feature.\n",
    "            threshold is the split threshold we want to use for calculating the entropy. \n",
    "        \"\"\"\n",
    "        \n",
    "        left_prop = sum([1 for val in values if val < threshold]) / len(values) \n",
    "        right_prop = 1 - left_prop    \n",
    "\n",
    "        left_labels = [\n",
    "            label for i, label in enumerate(lst) if values[i] < threshold\n",
    "        ]\n",
    "        right_labels = [\n",
    "            label for i, label in enumerate(lst) if values[i] >= threshold\n",
    "        ]\n",
    "\n",
    "        assert sorted(left_labels + right_labels) == sorted(lst)\n",
    "\n",
    "        left_entropy = self.entropy(left_labels)    \n",
    "        right_entropy = self.entropy(right_labels)   \n",
    "        \n",
    "        information_gain = (\n",
    "            self.entropy(lst) \n",
    "            - \n",
    "            (left_prop * left_entropy + right_prop * right_entropy)\n",
    "        )\n",
    "\n",
    "        return information_gain  \n",
    "    \n",
    "    def find_rules(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "            Helper function to find the split rules.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "        \"\"\"\n",
    "        rules = []        \n",
    "        for col in range(data.shape[1]):          \n",
    "            unique_values = np.unique(data[:, col])       \n",
    "            mid_points = [\n",
    "                (unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)\n",
    "            ]        \n",
    "            rules.append(mid_points)             \n",
    "        return rules\n",
    "    \n",
    "    def next_split(self, data, label):\n",
    "        \"\"\"\n",
    "            Helper function to find the split with most information gain, by using find_rules, and information gain.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            label contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        rules = self.find_rules(data)             \n",
    "        max_info = -1          \n",
    "        num_col = None          \n",
    "        threshold = None       \n",
    "        entropy_y = None   \n",
    "\n",
    "        \"\"\"\n",
    "            Check Number of features to use, None means all featurs. (Decision Tree always use all feature)\n",
    "            If n_features is a int, use n_features of features by random choice. \n",
    "            If n_features == 'sqrt', use sqrt(Total Number of Features ) by random choice.\n",
    "        \"\"\"\n",
    "        if num_col is None:\n",
    "            index_col = list(range(data.shape[1]))\n",
    "        else:\n",
    "            if num_col == 'sqrt': \n",
    "                num_index = int(np.sqrt(data.shape[1]))\n",
    "            else:\n",
    "                num_index = num_col\n",
    "            np.random.seed()  \n",
    "            index_col = np.random.choice(\n",
    "                data.shape[1], num_index, replace=False\n",
    "            )\n",
    "        \n",
    "        \"\"\"\n",
    "            Do the similar selection we did for features, n_split take in None or int or 'sqrt'.\n",
    "            For all selected feature and corresponding rules, we check it's information gain.       \n",
    "        \"\"\"\n",
    "        for i in index_col:\n",
    "\n",
    "            index_rules = []\n",
    "            num_rules = 0\n",
    "            \n",
    "            if self.n_split is None:\n",
    "                index_rules = rules[i]\n",
    "            elif len(rules[i]) > 0:\n",
    "                if self.n_split == 'sqrt':\n",
    "                    num_rules = int(np.sqrt(len(rules[i])))\n",
    "                else:\n",
    "                    num_rules = self.n_split\n",
    "                np.random.seed()\n",
    "                index_rules = np.random.choice(\n",
    "                    rules[i], num_rules, replace=False\n",
    "                )\n",
    "            \n",
    "            for rule in index_rules:\n",
    "                info = self.information_gain(label, data[:, i], rule)     \n",
    "                if info > max_info:  \n",
    "                    max_info = info\n",
    "                    num_col = i\n",
    "                    threshold = rule\n",
    "        \n",
    "        # print(f'threshold: {threshold}, label: {label[num_col]}')\n",
    "        return threshold, num_col\n",
    "        \n",
    "    def build_tree(self, X, y, depth):\n",
    "            \"\"\"\n",
    "                Helper function for building the tree.\n",
    "                \n",
    "                First build the root node.\n",
    "            \"\"\"\n",
    "            first_threshold, first_feature = self.next_split(X, y)\n",
    "            current = self.Node(first_threshold, first_feature)  \n",
    "            current.depth = depth\n",
    "            \n",
    "            if (self.max_depth is not None and depth > self.max_depth) \\\n",
    "                or (first_feature is None) \\\n",
    "                or (first_threshold is None):\n",
    "                current.predict = max(set(list(y)), key=list(y).count)\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            if len(np.unique(y)) == 1:\n",
    "                current.predict = y[0]\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            left_index = X[:, first_feature] <= first_threshold\n",
    "            right_index = X[:, first_feature] > first_threshold\n",
    "            \n",
    "            if len(left_index)==0 or len(right_index)==0:\n",
    "                current.predict = y[first_feature]\n",
    "                current.pure = True \n",
    "                return current\n",
    "            \n",
    "            \n",
    "            left_X, left_y = X[left_index,:], y[left_index]\n",
    "\n",
    "            current.left = self.build_tree(left_X, left_y, depth + 1)\n",
    "                \n",
    "            right_X, right_y = X[right_index,:], y[right_index]\n",
    "            current.right = self.build_tree(right_X, right_y, depth + 1)\n",
    "            \n",
    "            return current\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Decision Tree model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "\n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(np.array(X), np.array(y), 1)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \"\"\"\n",
    "        cur = self.root  \n",
    "        while not cur.pure:  \n",
    "            \n",
    "            feature = cur.feature  \n",
    "            threshold = cur.threshold \n",
    "            \n",
    "            if inp[feature] <= threshold:  \n",
    "                cur = cur.left\n",
    "            else:\n",
    "                cur = cur.right\n",
    "        return cur.predict\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "class RandomForest():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    RandomForest Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        n_trees: Number of trees. \n",
    "        trees: List store each individule tree\n",
    "        n_features: Number of features to use during building each individule tree.\n",
    "        n_split: Number of split for each feature.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = None, size_allowed = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Initilize all Attributes.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        \n",
    "    def bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Generates a bootstrap sample from the dataset (X, y).\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): 2D feature matrix where each row is a sample.\n",
    "            y (numpy.ndarray): 1D target array where each entry is a label.\n",
    "\n",
    "        Returns:\n",
    "            X_sample (numpy.ndarray): The bootstrapped 2D feature matrix.\n",
    "            y_sample (numpy.ndarray): The bootstrapped 1D target array.\n",
    "        \"\"\"\n",
    "        # Number of samples in X\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Generate random indices with replacement\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "\n",
    "        # Sample from X and y using the generated indices\n",
    "        X_sample = X[indices]\n",
    "        y_sample = np.array(y)[indices]\n",
    "\n",
    "        return X_sample, y_sample\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function fits the Random Forest model based on the training data. \n",
    "        \n",
    "        X_train is a matrix or 2-D numpy array, representing training instances. \n",
    "        Each training instance is a feature vector. \n",
    "        \n",
    "        y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        # self.for_running = y[4]\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            np.random.seed()\n",
    "            temp_clf = DecisionTree(\n",
    "                n_features=self.n_features, \n",
    "                n_split=self.n_split, \n",
    "                max_depth=self.max_depth, \n",
    "                size_allowed=self.size_allowed\n",
    "            )\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            temp_clf.fit(X_sample, y_sample)\n",
    "            self.trees.append(temp_clf)\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for tree in self.trees:\n",
    "            pred = tree.ind_predict(inp)\n",
    "            result.append(pred)\n",
    "\n",
    "        \n",
    "        labels, counts = np.unique(result, return_counts=True)\n",
    "        max_label_count = counts[np.argmax(counts)]\n",
    "        max_label_inds = np.where(counts == max_label_count)\n",
    "        if len(max_label_inds) > 1:\n",
    "            selected_index = np.random.choice(max_label_inds)\n",
    "            return [labels[selected_index]]\n",
    "        else:\n",
    "            return labels[np.argmax(counts)]\n",
    "    \n",
    "    def predict_all(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible combinations of 2 letters from the alphabet\n",
    "combinations = itertools.product(string.ascii_lowercase+'<>', repeat=2)\n",
    "pairs_ref = {}\n",
    "for i, combination in enumerate(combinations, start=0):  \n",
    "    combination_str = ''.join(combination)\n",
    "    pairs_ref[combination_str] = i\n",
    "\n",
    "\n",
    "combinations = itertools.product(string.ascii_lowercase, repeat=3)\n",
    "tripples_ref = {}\n",
    "for i, combination in enumerate(combinations, start=0): \n",
    "    combination_str = ''.join(combination)\n",
    "    tripples_ref[combination_str] = i\n",
    "\n",
    "\n",
    "def count_double_letters(word):\n",
    "    # Regular expression pattern to find double letters\n",
    "    pattern = r\"(.)\\1\"\n",
    "    matches = re.findall(pattern, word)\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "def vowel_proportion(word):\n",
    "    # Regular expression pattern to find vowels (a, e, i, o, u)\n",
    "    # Adding 'y' as a vowel as well, though it's sometimes considered a vowel\n",
    "    pattern = r\"[aeiouyAEIOUY]\"\n",
    "    \n",
    "    # Find all vowels in the word\n",
    "    vowels = re.findall(pattern, word)\n",
    "    vowel_count = len(vowels)\n",
    "    total_letters = len(word)\n",
    "    \n",
    "    proportion = vowel_count / max(total_letters, 1)\n",
    "    \n",
    "    return proportion\n",
    "\n",
    "\n",
    "def pair_count_vec(word):\n",
    "    vec = np.zeros(len(pairs_ref)+3)\n",
    "    # vec[-5] = tripples_ref[word[:3]]\n",
    "    vec[-2] = tripples_ref[word[-3:]]\n",
    "    # vec[-2] = vowel_proportion(word)\n",
    "    vec[-1] = count_double_letters(word)\n",
    "\n",
    "    word = f'<{word}>'\n",
    "    for i in range(1, len(word)):\n",
    "        index = pairs_ref[word[i-1:i+1]]\n",
    "        vec[index] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(train_words, train_labels, test_words):\n",
    "    train_vecs = np.vstack(pd.Series(train_words).apply(pair_count_vec))\n",
    "    test_vecs = np.vstack(pd.Series(test_words).apply(pair_count_vec))\n",
    "\n",
    "    clf = RandomForest(n_trees=10, n_split='sqrt')\n",
    "    clf.fit(train_vecs, np.array(train_labels))\n",
    "\n",
    "    return clf.predict_all(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8541666666666666"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classify(train_words, train_labels, test_words)\n",
    "\n",
    "(pred == np.array(test_labels)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
